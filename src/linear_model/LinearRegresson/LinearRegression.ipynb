{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f615b2",
   "metadata": {},
   "source": [
    "### Brief Explanation for Linear Regression \n",
    "\n",
    "This is where we find the best fit for our trueValue point\n",
    "\n",
    "Weights: This a parameter parameter that adjusts how much influence should the value of first neuron have on the proceeding neuron\n",
    "\n",
    "Bias: A systematic error that causes due to the wrong assumption of the model \n",
    "\n",
    "The difference between the true and the predicted value \n",
    "\n",
    "(We start with a random set of weights and baises that are then tweaked with the help of gradient decent  (An Optimization algorithem))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfc16e",
   "metadata": {},
   "source": [
    "# importing Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d14db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca600c",
   "metadata": {},
   "source": [
    "## The Code Implimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d97bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_regression(x, y, epochs=1000, lr_rate=1e-2): # Here x is the feature and y is the target while Epochs refer is each time we go through the data set and fine tune the weight and bias\n",
    "                                                        # Learning rate affects how fast or slow the model learns, If too high it takes bigger steps missing the best data point, if too low it may take too long to learn and possibly get stuck. Best to find the best balance \n",
    "\n",
    "    w = np.random.rand() # We start with a random weight and initially adjust it with gradient descent (Optimization algorithem). This determines how strong a input feature affects the target\n",
    "   \n",
    "    b = 0.0 # Similarly we start with the origin. The bias basically contorles the position of the line (if I'm not wrong). \n",
    "\n",
    "    for i in range(epochs): \n",
    "    \n",
    "        y_pred = np.dot(w, x) + b #Linear Regression Formula Y = Mx + b. here M = w, x = x, b = b. W contrls how much influence each feature has on the target value while the b helps to shift the position of the line\n",
    "        loss = np.mean((y_pred - y) **2) # Loss function to calculate the error MSE Mean Squared Error\n",
    "\n",
    "        w_gradient = 2 * np.mean((y_pred - y) * x) # Gradient descent an optimzation algorithem used to move the values of weight and biases to get a perfect value for the prediction to be accurate. We multiply x because it helps the model to learn and tells the model how each feature contribute and how each affect the accuracy\n",
    "\n",
    "        b_gradient = 2 * np.mean((y_pred - y))\n",
    "\n",
    "        w -= w_gradient * lr_rate # Moving in the direction where the loss is the least is why we use -= \n",
    "        b -= b_gradient * lr_rate\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch {i}: Prediction: {y_pred:}, Loss: {loss:.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82dd272",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a906d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Prediction: [0.48044331 0.96088661 1.44132992 1.92177323 2.40221653], Loss: 25.40\n",
      "Epoch 100: Prediction: [2.20143482 4.12426611 6.0470974  7.96992869 9.89275998], Loss: 0.01\n",
      "Epoch 200: Prediction: [2.14356628 4.08856673 6.03356718 7.97856763 9.92356808], Loss: 0.01\n",
      "Epoch 300: Prediction: [2.10232231 4.06312313 6.02392394 7.98472476 9.94552557], Loss: 0.00\n",
      "Epoch 400: Prediction: [2.07292698 4.04498901 6.01705103 7.98911305 9.96117508], Loss: 0.00\n",
      "Epoch 500: Prediction: [2.05197639 4.03206449 6.01215258 7.99224067 9.97232877], Loss: 0.00\n",
      "Epoch 600: Prediction: [2.03704453 4.02285295 6.00866137 7.99446979 9.98027821], Loss: 0.00\n",
      "Epoch 700: Prediction: [2.02640231 4.01628771 6.00617311 7.99605852 9.98594392], Loss: 0.00\n",
      "Epoch 800: Prediction: [2.01881741 4.01160855 6.00439969 7.99719083 9.98998197], Loss: 0.00\n",
      "Epoch 900: Prediction: [2.01341151 4.00827363 6.00313574 7.99799785 9.99285997], Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4, 5])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "linear_regression(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
